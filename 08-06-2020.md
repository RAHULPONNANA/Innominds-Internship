# linear Regression - Multiple variables

![alt text](https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w2_linear_regression_multiple/gradient_descent.png)

### Feature Scaling

to make sure the multiple variables/features are on the same scale, so that the contour plots are easier to plot between the features and the Gradient Descent Algorithm doesn't take much time.

x = x/max(x)

-1 <= x <= +1

### Mean Normalization

x = (x - mean) / (max(x) - min(x))

-0.5 <= x <= +0.5

### to make sure G.D algorithm works...

* after every iteration, J(t) decreases
* to make it easier, declare convergence if J(t) decreases by less than 10^-3 in one iteration.
* also, try alpha as ... 0,0001 , 0.001 , 0.01 , 0.1 , 1 , ...


### Solving for t using normal equation :

t = ( XTX }^-1 XTy

![alt text](https://i.stack.imgur.com/qzv5e.jpg)

![alt text](https://raw.githubusercontent.com/ritchieng/machine-learning-stanford/master/w2_linear_regression_multiple/normalequation.png)
