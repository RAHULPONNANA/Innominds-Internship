# Training a Neural Network

-> Randomly initialize weights

-> Implement forward propagation to get ht(x(i)) for any x(i)

-> Implement code to compute cost function J(t)

-> Implement backdrop to compute partial derivatives d/dt J(t)

-> Use Gradient checking to compare d/dt J(t) with numerical estimate of gradient of J(t).
Then disable gradient checking code.

-> Use gradient descent/advance optimization with backpropagation to minimize J(t).

# After Regularized Linear Expression

![alt](https://i1.wp.com/pingax.com/wp-content/uploads/2014/05/regularizataion.png)

## what to do?

-> more training examples

-> smaller sets of features

-> getting additional features

-> polynomial features

-> decreasing lambda

-> increasing lambda

**after training, test the 30% of data and calculate error**

![alt](https://slideplayer.com/slide/14472341/90/images/35/Model+selection+Choose.+How+well+does+the+model+generalize+Report+test+set+error+..jpg)

Cross Validation.

train-60%

cv-20%

test-20%
