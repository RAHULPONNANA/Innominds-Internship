# Training a Neural Network

-> Randomly initialize weights

-> Implement forward propagation to get ht(x(i)) for any x(i)

-> Implement code to compute cost function J(t)

-> Implement backdrop to compute partial derivatives d/dt J(t)

-> Use Gradient checking to compare d/dt J(t) with numerical estimate of gradient of J(t).
Then disable gradient checking code.

-> Use gradient descent/advance optimization with backpropagation to minimize J(t).
