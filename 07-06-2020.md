# linear regression

Training set --> Learning algorithm --> Hypothesis(h)


**eg.**

size of house --> h(x) --> Estimated price

**h(x) = t1.x + t2**

## Cost Function

square error function is also called Cost function.

![alt text](https://i.stack.imgur.com/tPhVh.png)

t1 - slope

t2 - intercept

### Gradient Descent Algiorithm

used for minimizing the cost function.

-> start with some t0,t1

-> keep changing t0,t1 to reduce J(to,t1)until we hopefully end up at a minimum.

![alt text](https://kousikk.files.wordpress.com/2014/11/screen-shot-2014-11-12-at-11-57-47-am.png)

![alt text](https://humanunsupervised.github.io/humanunsupervised.com/topics/images/lesson1/20.png)

### Batch Gradient Descent

each step of gradient descent uses all of the training examples.

The gradient descent equation, minimum can be solved by using the normal equations method also, but multiple steps gives the best result.
